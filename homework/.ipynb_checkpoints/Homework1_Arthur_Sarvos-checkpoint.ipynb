{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 3 Lab 1 - Git and Markdown\n",
    "\n",
    "## Homework:\n",
    "\n",
    "#### Setup\n",
    "* Resolve any installation issues before next class.\n",
    "* Make sure you have a github profile and created a repo called \"SYD_DAT_4\"\n",
    "* Clone the class repo (this one!)\n",
    "* Review this [code](../labs/Week 1/00_python_refresher.py) for a recap of some Python basics.\n",
    "\n",
    "#### Communication\n",
    "* Read [Analyzing the Analyzers](http://cdn.oreillystatic.com/oreilly/radarreport/0636920029014/Analyzing_the_Analyzers.pdf) for a useful look at the different types of data scientists. Write down 5 key points you took away from the article\n",
    "* Read about some [Markdown Techniques](http://daringfireball.net/projects/markdown/syntax)\n",
    "* Write a summary of 2 chapters of [The Data Science Handbook](http://www.thedatasciencehandbook.com/) in Markdown and submit a pull request in the Lab Directory\n",
    "\n",
    "#### Programming\n",
    "* Complete the lab from class and the additional Exercise below\n",
    "\n",
    "#### Course Project\n",
    "* Come up with 5 different ideas for your course project. For each one list:\n",
    "  * Overview of your idea\n",
    "  * What data you will use\n",
    "  * What the outcome is that you are trying to achieve\n",
    "  * Any ideas of modelling techniques it may involve\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework1_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Analyzers\n",
    "\n",
    "***\n",
    "\n",
    "### 5 Key Points:\n",
    "\n",
    "> **1)** There is a lot of miscommunication in the industry about what a Data Scientist is. This is both from the employee side and the employer side. The artcle indicates that the group of individuals when sitting interviews, did not feel they understood the scope of the job from the description.\n",
    "\n",
    "> **2)** There are a lot of different skillsets associated with Data Scientist. In the self survey conducted by the publishers, it showed how many who identified as Data Researchers where big on Statistics and Math Skills. The zone where Data Scientists often reside (Data Creative and Data Developer), there was a variety of skills identified they are very broad and the large number of differences in the experience of these types of people.\n",
    "\n",
    "> **3)** Kandel's survey illustrates the 3 broad categories of skillset which we identified in class: These were hacker, scripter and application user. The Hackers who were skilled at programming, scripters at the application of mathematical & statistical techniques, and application users, who were more into the visualisation and higher level analysis of the data, utilising a broad skillset.\n",
    "\n",
    "> **4)** Successful Data Scientists have a broad set of skills that allow them to build at lease a prototye version of all steps needed to derive insights from data. The best ones often have a very deep understanding of one or two aspects of data science (such as Statistics, Business Communcation, Coding) \n",
    "\n",
    "> **5)** The idea of a T-shaped skillset is very interesting. The top of the 'T' representing the breadth of skills across the board, and the vertical bar representing the depth in one particular area. Some ideas that have been put forward the the breadth of skills needed include: Data curation, analytics and visualisation, and networks and infrastructure.\n",
    "\n",
    ">> Excellent write up! I highly recommend reading Kandel's survey if you haven't already, it's a great study and good example on how to report interview studies. I'm glad you mentioned the T-shape, it's important to have a decent breadth of knowledge and then choosing one vertical that you'd like to specialise in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Science Handbook: Chapter Summaries\n",
    "***\n",
    "\n",
    "### Chapter 13 - Diane Wu (Data Scientist at Palantir) \n",
    "> Diane stresses the importance of the human-computer symbiosis (i.e. let the computer do what it does best E.g. crunch numbers/models and calculations, and let humans do what they do best, E.g. Interpret models and relationships to extract meaning from which to base a decision/recommendation.\n",
    "\n",
    "> \"Not all interesting problems produce insights, and not all interesting insights can inspire action that causes change\". This would come about in data science quite often I suspect. A business may be presented with a problem and in the end, we may not be able to produce a result. Understanding the scope of the issue is crucial to putting a solution forward.\n",
    "\n",
    "> The skills needed in data science largely depend on what you're wanting to do with it. Regardless, Diane considers the following as very important in the Data Scientist's toolbox\n",
    ">> Predictive Modelling\n",
    "\n",
    ">> Business Intelligence\n",
    "\n",
    ">> A spot in the middle where deeper questions about user behaviour are asked and teased out\n",
    "\n",
    "> Diane also suggests to approach the development as a data scientist using the T-shaped model (identified above). \n",
    "\n",
    ">> Great takeaways. Palantir is an interesting company and they are doing some intersting things with data science. Making sure you're project will prduce some sort of change is very important (at least as a way of prioritising your work).\n",
    "\n",
    "### Chapter 15 - Joe Blitzstein (Professor of Statistics at Harvard University)\n",
    "> \"Much of statisitics is about distinguishing signal from noise, distinguishing valid from invalid signals\". Here Joe emphasises that the Data Scientist (and Statistican for that matter), need to be able to discern what relationships are valid. There is a need to understand cause-effect relationships, probabilistic frameworks and content specific intelligence. \n",
    "\n",
    "> Expanding on Diane's point of view about the T-shaped model for a data scientist, Joe also states that different people can specialise in different parts of the process. So a team can all have similar top level skillsets (which helps them all to communicate with eachother) and also have different specialisations, which will assist the team to maintain a high performing, outcomes/results driven environment.\n",
    "\n",
    "> Visualisation, story telling and communcation are critical components of data science. Being able to build articulate visualisations enables users and decision makers to quickly assess models and their results. Often people who make the decisions dont have a lot of time to read the outputs, so being able to assist them with easily interpretable output is crucial to getting the right message across and influence the decision making process.\n",
    "\n",
    "> Learning data science is like learning a language. It has to be consistently approach. Learn something new everyday. Dont get into the mindset that until you know X,Y,Z you cant work in data science. It is important to have a good foundation, but it's also important to approach the subject matter incrementally. \n",
    "\n",
    ">> \"All models are wrong, but some are useful\" Finding the right model to use is not something that will occur on first try. A good approach is to start of basic and work your way into greater levels of complexity as dictated by the complexity of the problem at hand.\n",
    "\n",
    "> Knowledge of how the data is compiled is crucial. Something people often forget. This can introduce bias in the result. (eg. self selection bias, question response bias)\n",
    "\n",
    ">> Yes, the T shape is what forms a team with complimentary skill sets. All very applicable to work based problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible project topics\n",
    "\n",
    "### Survival Analysis - Using the Cox Regression Modelling framework with L1 regularization (Tibshirani):\n",
    "> An interesting paper by Rob Tibshirani on the use of the LASSO regularization technique on survival modelling. In particular its use in restricting coefficients in the COX regression model. Aim is to find a suitible dataset which will allow me to apply this method to better understand causal relationships that effect survival. I've just recently been doing some work on survival analysis so I thought this could be a neat application of a powerful method (L1 regularization) in order to build a more interpretable model.\n",
    "> http://www.google.com.au/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&ved=0ahUKEwia0MTU1OfLAhUkJaYKHQ5JDdwQFggtMAI&url=http%3A%2F%2Fstatweb.stanford.edu%2F~tibs%2Flasso%2Ffulltext.pdf&usg=AFQjCNF2J96-y6wxvdZ5Yr6Wpop7E_MXNQ&sig2=ahmJxf1u6Nte4Yzm_d2g8Q\n",
    "\n",
    ">> This would be a great project, very interesting! What sort of data do you have to use with it? I think the method would be great to try out - the key would be to find a nice dta set from industry that you could run it with.\n",
    "\n",
    "### E-M Algorithm - Fitting multivarite models to portfolios in order to calculate VaR and Expected Shortfall:\n",
    "> No sure if our course will be covering the use of Expectation Maximisation. Its analogy to clustering I find interesting and would like to use it in the implementation of a more robust framework for calculating Value at Risk and Expected shortfall. This could improve the somewhat simplistic and often dangerous approach to VaR calculations under Basel II. I would also like to extend the framework to caluclate expected shortfall for a given portfolio.\n",
    "\n",
    ">> I think it's a great idea, but not something we could do over the duration of the course. Would it be possible to break this down into something a bit smaller\n",
    "\n",
    "### Predicting Insurance - Using demographic and consumer behaviour data to maximise mailout takeup rates of insurance and minimise costs to the company (The CoIL Challenge 2000):\n",
    "> Being involved in Actuarial Studies, I'd like to re-attempt this competition using more up to date machine learning techniques to see if I could get a better prediction than the winner of this challenge. https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29\n",
    "\n",
    ">> This would be a good one.\n",
    "\n",
    "### Predicting Credit Card Defaults:\n",
    "> The following location has a lot of Machine Learning datasets, one of which relates to credit card defaults. In this preject I would go about comparing different methods and their ability to predict probabilities of default. https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    ">> This would be a great project to show your understanding of ml.\n",
    "\n",
    "### Algorithmic Trading (Past Kaggle Competition):\n",
    "> The aim of this competition was to determine the relationship between recent past order book events and future stock price mean reversion following shocks to liquidity. This is a little left of field but algorthmic trading is a fascinating field and would like to explore how I'd go about analysing this data and building a predictive model for trading. https://www.kaggle.com/c/AlgorithmicTradingChallenge\n",
    "\n",
    ">> This would be good - the trading applications are always a bit more difficult but doing it through a kaggle competition would be ok.\n",
    "\n",
    "> Overall this was a great submission Arthur - well done!!! Great written communicaiton skills (very important for data science)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Four - Movie Lens Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import panda library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each occupation in 'users', count the number of occurrences\n",
    "# Import the file\n",
    "names_col = ['user_id', 'age', 'gender', 'occupation', 'zip_code'] #define the columns\n",
    "users = pd.read_table('../labs/Week 1/u.user',\n",
    "                       sep='|',index_col='user_id',\n",
    "                       header=None ,\n",
    "                       names=names_col,\n",
    "                       dtype={'zip_code':str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# view the data (first 10 obs)\n",
    "users.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each occupation, calculate the mean age\n",
    "users.groupby(by=['occupation'])['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each occupation, calculate the minimum and maximum ages\n",
    "min_ages = users.groupby(by=['occupation'])['age'].min()\n",
    "max_ages = users.groupby(by=['occupation'])['age'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_ages # View data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_ages # view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each combination of occupation and gender, calculate the mean age\n",
    "users.groupby(['occupation','gender'])['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomly sample a DataFrame\n",
    "sample_n = users.sample(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_n.shape # confirm sample size (expecting 50 x 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# detect duplicate users\n",
    "# first lets sum the number of duplicates that result in True\n",
    "sum(users.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dups = users[users.duplicated()==True] # subset where the duplicate flag is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dups # view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dups.shape # check dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniques = users[users.duplicated()==False] # look at the unique values and check dims for clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniques.shape #check dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users.shape #Should have all observations so check original shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises from the lab (Week 1) \n",
    "### Using the drinks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drinks = pd.read_csv('../labs/Week 1/drinks.csv') #import\n",
    "drinks.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter DataFrame to only include European countries\n",
    "euro = drinks[drinks.continent=='EU']\n",
    "euro.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter DataFrame to only include European countries with wine_servings > 300\n",
    "euro_wine = euro[euro.wine_servings > 300]\n",
    "euro_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alternative approach\n",
    "drinks[(drinks.continent == 'EU') & (drinks.wine_servings > 300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the average 'beer_servings' for all of Europe\n",
    "drinks[drinks.continent =='EU'].beer_servings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine which 10 countries have the highest total_litres_of_pure_alcohol\n",
    "# Sort the data and take the head\n",
    "drinks.sort_values(by='total_litres_of_pure_alcohol',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename the column 'beer_servings' to 'beer'\n",
    "drinks.rename(columns={'beer_servings': 'beer'},inplace=True)\n",
    "drinks.rename(columns={'wine_servings': 'wine'},inplace=True)\n",
    "drinks.rename(columns={'spirit_servings': 'spirits'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a new column as a function of existing columns, total_servings = beer + wine + spirits\n",
    "drinks['total_servings'] = drinks.beer + drinks.wine + drinks.spirits\n",
    "drinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the column you just added\n",
    "del drinks['total_servings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the column is gone\n",
    "drinks.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
